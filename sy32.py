import numpy as np
import matplotlib.pyplot as plt

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# è¯»å–æ•°æ®
load_data1 = np.loadtxt('experiment_03_training_set.csv', delimiter=',')
load_data2 = np.loadtxt('experiment_03_testing_set.csv', delimiter=',')

train_x = load_data1[:, :-1]
train_y = load_data1[:, -1].reshape([load_data1.shape[0], 1])
test_x = load_data2[:, :-1]
test_y = load_data2[:, -1].reshape([load_data2.shape[0], 1])

# æ·»åŠ åç½®é¡¹
train_x = np.hstack([np.ones((train_x.shape[0], 1)), train_x])
test_x = np.hstack([np.ones((test_x.shape[0], 1)), test_x])

print(f"è®­ç»ƒé›†å¤§å°: {train_x.shape[0]}, ç‰¹å¾æ•°: {train_x.shape[1]-1}")
print(f"æµ‹è¯•é›†å¤§å°: {test_x.shape[0]}")

# å®šä¹‰sigmoidå‡½æ•°
def sigmoid(z):
    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))

# å®šä¹‰æŸå¤±å‡½æ•°
def compute_loss(y, y_pred):
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))

# é€»è¾‘å›å½’è®­ç»ƒå‡½æ•°
def logistic_regression(X, y, learning_rate=0.001, num_iterations=1000):
    w = np.zeros((X.shape[1], 1))
    losses = []
    
    for i in range(num_iterations):
        # å‰å‘ä¼ æ’­
        z = np.dot(X, w)
        y_pred = sigmoid(z)
        
        # è®¡ç®—æŸå¤±
        loss = compute_loss(y, y_pred)
        losses.append(loss)
        
        # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰
        dw = np.dot(X.T, (y_pred - y)) / len(y)
        w -= learning_rate * dw
        
        if i % 200 == 0:
            print(f'è¿­ä»£æ¬¡æ•° {i:3d}, æŸå¤±: {loss:.4f}')
    
    return w, losses

# é¢„æµ‹å‡½æ•°
def predict(X, w, threshold=0.5):
    y_pred_prob = sigmoid(np.dot(X, w))
    y_pred = (y_pred_prob >= threshold).astype(int)
    return y_pred, y_pred_prob

# æ‰‹åŠ¨è®¡ç®—æ··æ·†çŸ©é˜µå’Œè¯„ä¼°æŒ‡æ ‡
def calculate_metrics(y_true, y_pred):
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()
    
    # è®¡ç®—æ··æ·†çŸ©é˜µ
    TP = np.sum((y_true == 1) & (y_pred == 1))
    TN = np.sum((y_true == 0) & (y_pred == 0))
    FP = np.sum((y_true == 0) & (y_pred == 1))
    FN = np.sum((y_true == 1) & (y_pred == 0))
    
    cm = np.array([[TN, FP], [FN, TP]])
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    accuracy = (TP + TN) / (TP + TN + FP + FN)
    error_rate = 1 - accuracy
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    return cm, accuracy, error_rate, precision, recall, f1, TP, TN, FP, FN

# æ‰‹åŠ¨ç»˜åˆ¶æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
def plot_confusion_matrix_manual(cm, ax):
    # åˆ›å»ºçƒ­åŠ›å›¾æ•ˆæœ
    im = ax.imshow(cm, cmap='Blues', interpolation='nearest', vmin=0, vmax=cm.max()*1.1)
    
    # è®¾ç½®åæ ‡è½´
    ax.set_xticks([0, 1])
    ax.set_yticks([0, 1])
    ax.set_xticklabels(['é¢„æµ‹åä¾‹', 'é¢„æµ‹æ­£ä¾‹'], fontsize=10)
    ax.set_yticklabels(['çœŸå®åä¾‹', 'çœŸå®æ­£ä¾‹'], fontsize=10)
    
    # åœ¨æ¯ä¸ªæ ¼å­ä¸­æ·»åŠ æ•°å€¼
    for i in range(2):
        for j in range(2):
            color = 'white' if cm[i, j] > cm.max()/2 else 'black'
            ax.text(j, i, str(cm[i, j]), 
                   ha="center", va="center", 
                   color=color, fontsize=14, fontweight='bold')
    
    ax.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12, fontweight='bold')
    ax.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12, fontweight='bold')
    ax.set_title('æ··æ·†çŸ©é˜µ', fontsize=14, fontweight='bold')
    
    # æ·»åŠ é¢œè‰²æ¡
    plt.colorbar(im, ax=ax)

# è®­ç»ƒæ¨¡å‹
print("=" * 60)
print("å¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
print("=" * 60)
w, losses = logistic_regression(train_x, train_y, learning_rate=0.001, num_iterations=1000)

# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
print("\nåœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹...")
y_pred, y_pred_prob = predict(test_x, w)

# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
cm, accuracy, error_rate, precision, recall, f1, TP, TN, FP, FN = calculate_metrics(test_y, y_pred)

# åˆ›å»ºä¸“ä¸šçš„æ•°æ®å¯è§†åŒ–
fig = plt.figure(figsize=(18, 6))

# 1. æŸå¤±å‡½æ•°æ›²çº¿
ax1 = plt.subplot(1, 3, 1)
plt.plot(losses, color='#E74C3C', linewidth=2.5, alpha=0.8)
plt.fill_between(range(len(losses)), losses, alpha=0.3, color='#E74C3C')
plt.title('æŸå¤±å‡½æ•°æ”¶æ•›æ›²çº¿', fontsize=16, fontweight='bold', pad=20)
plt.xlabel('è¿­ä»£æ¬¡æ•°', fontsize=12, fontweight='bold')
plt.ylabel('äº¤å‰ç†µæŸå¤±', fontsize=12, fontweight='bold')
plt.grid(True, alpha=0.4)

# æ·»åŠ æœ€ç»ˆæŸå¤±å€¼æ ‡æ³¨
final_loss = losses[-1]
plt.annotate(f'æœ€ç»ˆæŸå¤±: {final_loss:.4f}', 
             xy=(len(losses)-1, final_loss), 
             xytext=(len(losses)*0.6, losses[0]*0.8),
             arrowprops=dict(arrowstyle='->', color='black', alpha=0.7),
             fontsize=11, fontweight='bold',
             bbox=dict(boxstyle="round,pad=0.3", facecolor="lightyellow"))

# 2. æ··æ·†çŸ©é˜µ
ax2 = plt.subplot(1, 3, 2)
plot_confusion_matrix_manual(cm, ax2)

# 3. è¯„ä¼°æŒ‡æ ‡é›·è¾¾å›¾
ax3 = plt.subplot(1, 3, 3, polar=True)

# é›·è¾¾å›¾æ•°æ®
categories = ['ç²¾åº¦', 'æŸ¥å‡†ç‡', 'æŸ¥å…¨ç‡', 'F1-score', 'é”™è¯¯ç‡']
values = [accuracy, precision, recall, f1, error_rate]
N = len(categories)

# è§’åº¦è®¡ç®—
angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
values += values[:1]  # é—­åˆé›·è¾¾å›¾
angles += angles[:1]

# ç»˜åˆ¶é›·è¾¾å›¾
ax3.plot(angles, values, 'o-', linewidth=2, color='#2E86AB', markersize=8)
ax3.fill(angles, values, alpha=0.25, color='#2E86AB')

# è®¾ç½®é›·è¾¾å›¾æ ‡ç­¾
ax3.set_xticks(angles[:-1])
ax3.set_xticklabels(categories, fontsize=11, fontweight='bold')
ax3.set_ylim(0, 1)
ax3.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])
ax3.grid(True, alpha=0.3)
plt.title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾', fontsize=16, fontweight='bold', pad=20)

# åœ¨é›·è¾¾å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡æ³¨
for angle, value, category in zip(angles[:-1], values[:-1], categories):
    ax3.annotate(f'{value:.3f}', 
                xy=(angle, value), 
                xytext=(5, 5), 
                textcoords='offset points',
                fontsize=10, 
                fontweight='bold',
                bbox=dict(boxstyle="round,pad=0.3", facecolor='yellow', alpha=0.7))

plt.tight_layout()
plt.show()

# æ‰“å°è¯¦ç»†çš„æ–‡æœ¬ç»“æœ
print("\n" + "="*70)
print("ğŸ“Š é€»è¾‘å›å½’æ¨¡å‹å®éªŒç»“æœ")
print("="*70)

print(f"\nğŸ¯ æ¨¡å‹å‚æ•°:")
print(f"   å­¦ä¹ ç‡: 0.001")
print(f"   è¿­ä»£æ¬¡æ•°: 1000")
print(f"   åˆå§‹æƒé‡: [0, 0, ..., 0]")
print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.6f}")

print(f"\nğŸ“ˆ è®­ç»ƒè¿‡ç¨‹:")
print(f"   åˆå§‹æŸå¤±: {losses[0]:.4f}")
print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
print(f"   æŸå¤±å‡å°‘: {losses[0]-losses[-1]:.4f} ({((losses[0]-losses[-1])/losses[0]*100):.1f}%)")

print(f"\nğŸ” æ··æ·†çŸ©é˜µè¯¦æƒ…:")
print("   " + " "*15 + "é¢„æµ‹ç»“æœ")
print("   " + " "*15 + "æ­£ä¾‹" + " "*8 + "åä¾‹")
print("   " + "çœŸå®æƒ…å†µ æ­£ä¾‹" + f"    {TP:4d} (TP)" + f"    {FN:4d} (FN)")
print("   " + "çœŸå®æƒ…å†µ åä¾‹" + f"    {FP:4d} (FP)" + f"    {TN:4d} (TN)")

print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
print(f"   âœ… ç²¾åº¦ (Accuracy):  {accuracy:.4f}")
print(f"   âŒ é”™è¯¯ç‡ (Error Rate): {error_rate:.4f}")
print(f"   ğŸ¯ æŸ¥å‡†ç‡ (Precision): {precision:.4f}")
print(f"   ğŸ” æŸ¥å…¨ç‡ (Recall):    {recall:.4f}")
print(f"   âš–ï¸  F1-score:        {f1:.4f}")

print(f"\nğŸ“‹ è¯¦ç»†ç»Ÿè®¡:")
total_samples = len(test_y)
print(f"   æ€»æµ‹è¯•æ ·æœ¬æ•°: {total_samples}")
print(f"   æ­£ç¡®é¢„æµ‹æ•°: {TP + TN}")
print(f"   é”™è¯¯é¢„æµ‹æ•°: {FP + FN}")
print(f"   æ­£ä¾‹æ ·æœ¬æ•°: {TP + FN}")
print(f"   åä¾‹æ ·æœ¬æ•°: {TN + FP}")

# æ˜¾ç¤ºæ¨¡å‹å‚æ•°
print(f"\nğŸ”§ æ¨¡å‹æƒé‡:")
print(f"   åç½®é¡¹ (w0): {w[0,0]:.4f}")
for i in range(1, min(6, len(w))):  # åªæ˜¾ç¤ºå‰5ä¸ªç‰¹å¾æƒé‡
    print(f"   ç‰¹å¾{w.shape[0]-1}æƒé‡ (w{i}): {w[i,0]:.4f}")
if len(w) > 6:
    print(f"   ... (å…± {w.shape[0]-1} ä¸ªç‰¹å¾)")

# æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„é¢„æµ‹è¯¦æƒ…
print(f"\nğŸ” å‰10ä¸ªæµ‹è¯•æ ·æœ¬é¢„æµ‹è¯¦æƒ…:")
print("æ ·æœ¬\tçœŸå®æ ‡ç­¾\té¢„æµ‹æ¦‚ç‡\té¢„æµ‹æ ‡ç­¾\tæ˜¯å¦æ­£ç¡®")
print("-" * 55)
for i in range(min(10, len(test_y))):
    true_label = test_y[i][0]
    pred_prob = y_pred_prob[i][0]
    pred_label = y_pred[i][0]
    is_correct = "âœ“" if true_label == pred_label else "âœ—"
    print(f"{i+1:2d}\t{true_label:2d}\t\t{pred_prob:.4f}\t\t{pred_label:2d}\t\t{is_correct}")

# é¢å¤–ç»˜åˆ¶é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒå›¾
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
# æŒ‰çœŸå®ç±»åˆ«åˆ†åˆ«ç»˜åˆ¶æ¦‚ç‡åˆ†å¸ƒ
mask_positive = (test_y.flatten() == 1)
mask_negative = (test_y.flatten() == 0)

plt.hist(y_pred_prob[mask_positive], bins=20, alpha=0.7, color='red', 
         label='çœŸå®æ­£ä¾‹', edgecolor='black')
plt.hist(y_pred_prob[mask_negative], bins=20, alpha=0.7, color='blue', 
         label='çœŸå®åä¾‹', edgecolor='black')
plt.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='å†³ç­–è¾¹ç•Œ')
plt.xlabel('é¢„æµ‹æ¦‚ç‡')
plt.ylabel('é¢‘æ•°')
plt.title('é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼ˆæŒ‰çœŸå®ç±»åˆ«ï¼‰')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
# æŒ‡æ ‡å¯¹æ¯”æ¡å½¢å›¾
metrics_names = ['ç²¾åº¦', 'æŸ¥å‡†ç‡', 'æŸ¥å…¨ç‡', 'F1']
metrics_values = [accuracy, precision, recall, f1]
colors = ['#2ecc71', '#3498db', '#9b59b6', '#e74c3c']

bars = plt.bar(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')
plt.ylim(0, 1)
plt.title('æ¨¡å‹è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”')
plt.ylabel('å¾—åˆ†')
plt.grid(True, alpha=0.3)

# åœ¨æ¡å½¢å›¾ä¸Šæ·»åŠ æ•°å€¼æ ‡ç­¾
for bar, value in zip(bars, metrics_values):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

print("\n" + "="*70)
print("ğŸ‰ å®éªŒå®Œæˆï¼")
print("="*70)